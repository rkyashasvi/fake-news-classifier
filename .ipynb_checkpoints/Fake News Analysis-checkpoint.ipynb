{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "Unnamed: 0                                                      \n",
       "8476                             You Can Smell Hillary’s Fear   \n",
       "10294       Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608              Kerry to go to Paris in gesture of sympathy   \n",
       "10142       Bernie supporters on Twitter erupt in anger ag...   \n",
       "875          The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                         text label  \n",
       "Unnamed: 0                                                           \n",
       "8476        Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294       Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608        U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142       — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875         It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('fake_or_real_news.csv')\n",
    "data = data.set_index(\"Unnamed: 0\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "Unnamed: 0                                                      \n",
       "8476                             You Can Smell Hillary’s Fear   \n",
       "10294       Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608              Kerry to go to Paris in gesture of sympathy   \n",
       "10142       Bernie supporters on Twitter erupt in anger ag...   \n",
       "875          The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                         text label  \n",
       "Unnamed: 0                                                           \n",
       "8476        Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294       Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608        U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142       — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875         It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.drop(\"label\",axis=1)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['text'] = y['text'].apply(lambda x: ''.join([\" \" if ord(i) < 32 or (ord(i) > 32 and ord(i) < 65) or (ord(i) > 90 and ord(i) < 97) or ord(i) > 122 else i for i in x]))\n",
    "y['title'] = y['title'].apply(lambda x: ''.join([\" \" if ord(i) < 32 or (ord(i) > 32 and ord(i) < 65) or (ord(i) > 90 and ord(i) < 97) or ord(i) > 122 else i for i in x]))\n",
    "\n",
    "#filter out numbers and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>you can smell hillary s fear</td>\n",
       "      <td>daniel greenfield  a shillman journalism fello...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>kerry to go to paris in gesture of sympathy</td>\n",
       "      <td>u s  secretary of state john f  kerry said mon...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>kaydee king   kaydeeking  november         t...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>the battle of new york  why this primary matters</td>\n",
       "      <td>it s primary day in new york and front runners...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "Unnamed: 0                                                      \n",
       "8476                             you can smell hillary s fear   \n",
       "10294       watch the exact moment paul ryan committed pol...   \n",
       "3608              kerry to go to paris in gesture of sympathy   \n",
       "10142       bernie supporters on twitter erupt in anger ag...   \n",
       "875          the battle of new york  why this primary matters   \n",
       "\n",
       "                                                         text label  \n",
       "Unnamed: 0                                                           \n",
       "8476        daniel greenfield  a shillman journalism fello...  fake  \n",
       "10294       google pinterest digg linkedin reddit stumbleu...  fake  \n",
       "3608        u s  secretary of state john f  kerry said mon...  real  \n",
       "10142         kaydee king   kaydeeking  november         t...  fake  \n",
       "875         it s primary day in new york and front runners...  real  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['text'] = y['text'].str.lower()\n",
    "y['title'] = y['title'].str.lower()\n",
    "y['label'] = y['label'].str.lower()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mazhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # You may need to run this first\n",
    "stop_array = (stopwords.words('english'))\n",
    "#set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words\n",
    "y['text'] = y['text'].apply(lambda row: ' '.join([\" \" if word in stop_array else word for word in row.split()]))\n",
    "y['title'] = y['title'].apply(lambda row: ' '.join([\" \" if word in stop_array else word for word in row.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/mazhang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all non-English words. This is a problem because it also deletes names not in the corpus\n",
    "# Could keep as an option to see if it increases accuracy\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "opt = y.copy()\n",
    "words = set(words.words())\n",
    "opt['text'] = opt['text'].apply(lambda row: ' '.join([\" \" if w not in words else w for w in row.split()]))\n",
    "opt['title'] = opt['title'].apply(lambda row: ' '.join([\" \" if w not in words else w for w in row.split()]))\n",
    "opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.head()\n",
    "# TODO: Might need to lemmatize words before vectorizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y.copy()\n",
    "# With text\n",
    "X_train, X_test, y_train, y_test = train_test_split(y['text'], y2, test_size=0.33, random_state=53)\n",
    "\n",
    "# With headlines\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(y['title'], y2, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf-idf and count vectorizer to find most relevant words in corpus\n",
    "# TfidfVectorizer should remove stop words and words that appear in more than 70% of the articles\n",
    "\n",
    "# Vectorizers have an ngram range! Check if bigrams will improve accuracy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english') # already gets rid of stop words\n",
    "count_train = count_vectorizer.fit_transform(X_train) \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)  \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[-10:])\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_df.equals(tfidf_df)) # check if the vectorizers extracted different tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# MultinomialNB\n",
    "multi_nb = MultinomialNB()\n",
    "np.shape(tfidf_train)\n",
    "np.shape(y_train)\n",
    "multi_nb.fit(tfidf_train, y_train['label'])\n",
    "pred = multi_nb.predict(tfidf_test)\n",
    "score = accuracy_score(y_test['label'], pred)\n",
    "print(\"accuracy:   %0.3f\" % score) # Multinomial Naive Bayes accuracy: 0.857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a confusion matrix to compare accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test['label'], pred, labels=['fake', 'real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, classes=['fake', 'real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_multi_nb = MultinomialNB()\n",
    "count_multi_nb.fit(count_train,y_train['label'])\n",
    "c_pred = count_multi_nb.predict(count_test)\n",
    "c_score = accuracy_score(y_test['label'],pred)\n",
    "print(\"accuracy:   %0.3f\" % c_score)\n",
    "# this isn't outperforming tf-idf for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "linear_clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "linear_clf.fit(tfidf_train, y_train['label'])\n",
    "pred = linear_clf.predict(tfidf_test)\n",
    "linear_score = accuracy_score(y_test['label'], pred)\n",
    "print(\"accuracy:   %0.3f\" % linear_score)\n",
    "linear_cm = confusion_matrix(y_test['label'], pred, labels=['fake', 'real'])\n",
    "plot_confusion_matrix(linear_cm, classes=['fake', 'real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=0.1)\n",
    "last_score = 0\n",
    "for alpha in np.arange(0,1,.1):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    nb_classifier.fit(tfidf_train, y_train['label'])\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    score = accuracy_score(y_test['label'], pred)\n",
    "    if score > last_score:\n",
    "        clf = nb_classifier\n",
    "    print(\"Alpha: {:.2f} Score: {:.5f}\".format(alpha, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative(vectorizer, classifier, n=100):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    for coef,feat in topn_class1:\n",
    "        print(class_labels[0],coef,feat)    \n",
    "    print()\n",
    "    for coef,feat in reversed(topn_class2):\n",
    "        print(class_labels[1],coef,feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_informative(tfidf_vectorizer,linear_clf,n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most real\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "sorted(zip(clf.coef_[0], feature_names), reverse=True)[:20] # zip coefficients and sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most fake\n",
    "sorted(zip(clf.coef_[0], feature_names))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_with_weights = sorted(list(zip(feature_names, clf.coef_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english', non_negative=True)\n",
    "hash_train = hash_vectorizer.fit_transform(X_train)\n",
    "hash_test = hash_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(hash_train, y_train['label'])\n",
    "pred = clf.predict(hash_test)\n",
    "score = accuracy_score(y_test['label'], pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "cm = confusion_matrix(y_test['label'], pred, labels=['fake', 'real'])\n",
    "plot_confusion_matrix(cm, classes=['fake', 'real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining fake news with simple bag-of-words or TF-IDF vectors is an oversimplified approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up how to make an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Other types of Naive Bayes, Logistic Regression, and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_clf = LogisticRegression()\n",
    "logistic_clf.fit(tfidf_train, y_train['label'])\n",
    "pred = logistic_clf.predict(tfidf_test)\n",
    "logistic_score = accuracy_score(y_test['label'], pred)\n",
    "print(\"accuracy:   %0.3f\" % logistic_score)\n",
    "logistic_cm = confusion_matrix(y_test['label'], pred, labels=['fake', 'real'])\n",
    "plot_confusion_matrix(logistic_cm, classes=['fake', 'real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations of fake news categories\n",
    "# Features most associated with the different fake news categories\n",
    "# Domain names, sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sample = y['text'][0:y.shape[0]].str.cat(sep=' ') # Dump sample vocab for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('articles.txt','w',encoding='utf-8')\n",
    "f.write(dict_sample)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = LineSentence('articles.txt')\n",
    "\n",
    "# learn the dictionary \n",
    "article_dict = Dictionary(sent)\n",
    "    \n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "article_dict.filter_extremes(no_below=5, no_above=0.2)\n",
    "article_dict.compactify()\n",
    "\n",
    "article_dict.save('articles.dict')\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "article_dict = Dictionary.load('articles.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(filepath,d): # output bag of words representation\n",
    "    for review in LineSentence(filepath):\n",
    "        yield d.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate bag-of-words representations for all reviews and save them as a matrix\n",
    "MmCorpus.serialize('articles.mm',\n",
    "                       bow('articles.txt',article_dict))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "corpus = MmCorpus('articles.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model (Runs outside of Jupyter for some reason)\n",
    "# lda = LdaMulticore(corpus,num_topics=10,\n",
    "#                    id2word=article_dict, \n",
    "#                    workers=2)\n",
    "    \n",
    "# lda.save('./lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaMulticore.load('./lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics (topic_number, lda_model, topn=5):\n",
    "    print(u'{:10} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=25):\n",
    "        print(u'{:10} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(2,lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warning\n",
    "# LDA is a transformation from bag-of-words counts into a topic space of lower dimensionality. It can be thought of as probability distributions over words.\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda, corpus,\n",
    "                                              article_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for general corpus\n",
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = y.copy()\n",
    "fake = fake.loc[fake['label'] == 'fake']\n",
    "real = y.copy()\n",
    "real = real.loc[real['label'] == 'real']\n",
    "fake_text = fake['text'][0:fake.shape[0]].str.cat(sep=' ')\n",
    "real_text = real['text'][0:real.shape[0]].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('fake.txt','w',encoding='utf-8')\n",
    "f1.write(fake_text)\n",
    "f1.close()\n",
    "f2 = open('real.txt','w',encoding='utf-8')\n",
    "f2.write(real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sent = LineSentence('fake.txt')\n",
    "fake_dict = Dictionary(fake_sent)\n",
    "fake_dict.filter_extremes(no_below=5, no_above=0.2)\n",
    "fake_dict.compactify()\n",
    "fake_dict.save('fake.dict')\n",
    "fake_dict = Dictionary.load('fake.dict')\n",
    "\n",
    "real_sent = LineSentence('real.txt')\n",
    "real_dict = Dictionary(real_sent)\n",
    "real_dict.filter_extremes(no_below=5, no_above=0.2)\n",
    "real_dict.compactify()\n",
    "real_dict.save('real.dict')\n",
    "real_dict = Dictionary.load('real.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MmCorpus.serialize('fake.mm', bow('fake.txt',fake_dict))\n",
    "fake_corpus = MmCorpus('fake.mm')\n",
    "MmCorpus.serialize('real.mm', bow('real.txt',real_dict))\n",
    "real_corpus = MmCorpus('real.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might have to run this outside of Jupyter\n",
    "# fake_lda = LdaMulticore(fake_corpus,num_topics=10,id2word=fake_dict,workers=2)  \n",
    "# fake_lda.save('./fake_lda_model')\n",
    "\n",
    "# real_lda = LdaMulticore(real_corpus,num_topics=10,id2word=real_dict,workers=2)\n",
    "# real_lda.save('./real_lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_lda = LdaMulticore.load('./fake_lda_model')\n",
    "real_lda = LdaMulticore.load('./real_lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(2, fake_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(2, real_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_LDAvis = pyLDAvis.gensim.prepare(fake_lda, fake_corpus, fake_dict)\n",
    "real_LDAvis = pyLDAvis.gensim.prepare(real_lda, real_corpus, real_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(fake_LDAvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(real_LDAvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HTML files\n",
    "pyLDAvis.save_html(fake_LDAvis,'fake_lda.html')\n",
    "pyLDAvis.save_html(real_LDAvis,'real_lda.html')\n",
    "pyLDAvis.save_html(LDAvis_prepared,'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset = pd.read_csv('fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset.groupby(['type']).size().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stop words from larger corpora\n",
    "# This will take a lot of time because there's a lot of data\n",
    "\n",
    "for token in headline_text:\n",
    "    if token in stop_array:\n",
    "        headline_text.remove(token)\n",
    "        \n",
    "for token in article_text:\n",
    "    if token in stop_array:\n",
    "        article_text.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag and then stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the words\n",
    "# Might need to tag parts of speech first and lemmatize based on that\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "headline_lemma = []\n",
    "article_lemma = []\n",
    "\n",
    "for token in headline_text:\n",
    "    headline_lemma.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "for token in article_text:\n",
    "    article_lemma.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "# Parse out words that appear too often\n",
    "\n",
    "fdist1 = FreqDist(headline_text)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "make_pipeline(CountVectorizer(stop_words='english'),MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dummy variables\n",
    "copy = tfidf_df.copy()\n",
    "#copy['label'] = copy['label'].apply(lambda x: 1 if x == 'FAKE' else 0)\n",
    "\n",
    "# Import into a pickle model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(multi_nb, 'model.pkl')\n",
    "model_columns = list(copy.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
